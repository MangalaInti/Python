Import sys

dir(sys)

help(sys.getsizeof)

Spark do it in memory, Hadoop has to read from disk and write to disk

https://colab.research.google.com

Sample Data
https://lnkd.in/g-GxEYV

GitHub:main > Apache_Pyspark_by_Example.ipynb

https://lnkd.in/g-GxEYV


--Working Dataframe API
Dataframes are High_level API
Resilient Distributed Datasets(RDD's) - Low-level API
